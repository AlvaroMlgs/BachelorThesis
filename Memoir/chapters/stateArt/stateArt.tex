
\let\textcircled=\pgftextcircled
\chapter{State of the art} \label{chap:sota}

\initial{T}he relatively recent history of ``small'' (considered as weighting less than 25 kg) commercial UAVs implies that the technologies involved have not reached a high Technology Readiness Level (TRL) \cite{assistantsecretaryofdefenseforresearchandengineering2011} until very recently, or are still under development (TRL 6-7).
For that reason, most of the work considered in the present chapter is not yet ready for the market and has a great capacity to improve.
Universities are actively exploring the autonomous aptitudes of UAVs, but the economical exploitation potential is still small due to the low reliability issues that these systems are encountering in non-controlled environments.

% In the present chapter several fields of interest to the project will be explored.
% First, some environment sensing alternatives will be compared.
% Then, the current commercial implementations of collision avoidance on aircraft will be studied, to finalise with ongoing research at the leading universities in UAV technology development.


\section{Environment sensing} \label{sec:sensing}

Knowing the environment is the initial step towards any interaction of a system with its surroundings.
That knowledge could in principle be acquired a-priori and then copied into the system, but when either time-changing or completely unknown scenarios are considered, having a means of sensing the environment is essential.

\subsection{Radar}

Radar stands for RAdio Detection And Ranging.
Its usage dates back to 1904, when H\"ulsmeyer registered the patent for his Telemobiloscope \cite{hulsmeyer1904}, using a primitive variant of the radar for detecting metallic ships.

Nowadays, the radar works by emitting one powerful radio signal impulse against the object that needs to be located.
If the signal reaches a solid object on its path, the electromagnetic wave will be distorted and part of it will be reflected back to the emission point, where a second listening antenna can detect it.

The distance of the detected object is calculated by measuring the flight time of the signal since it was emitted until it is detected, applying the signal propagation speed ($3 \times 10^{^8} m/s$ for electromagnetic waves) as correction factor.
The azimuth position of the object can be estimated if the initial signal was created by a highly directional antenna, since the orientation of the transmitting antenna can be measured, and the sensed object must be contained within the electromagnetic radiation field.

The fact that radar uses electromagnetic waves implies that the response time can be very low, although some technical issues may arise for the same fact at the time of processing the returned signal.
For that reason, radar-based sensor systems focus on temporal processing techniques to improve the results of the measurements \cite{krolik2005}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.3\textwidth]{./figures/antennaPattern.jpg}
	\caption{Highly directional radiation pattern. {\footnotesize Source: \url{cisco.com}}}
	\label{fig:antenna}
\end{figure}

\subsection{Sonar}

The ultrasonic rangefinder (commonly known as sonar, for SOund Navigation And Ranging) relies, like the radar, on the measurement of the flight time of a signal rebounding against the target.
However, instead of being electromagnetic waves, the carriers are sound waves, with a frequency usually beyond the human hearing range upper threshold (hence ultrasonic).

It is important to mention that the calculation of distance from the flight time of the rebounded signal is to some degree dependent on the environment:
The speed of propagation of sound depends on the temperature of the medium through which it propagates as $a=\sqrt{\gamma R_g T}$.
Thus, the temperature of the air should be monitored to compensate for variations during the flight.
Fortunately, the error associated with temperature changes around room temperature is of the same order than the accuracy of the sensor itself, so it is safe to consider ambient temperature at the initial calibration stage only and assume it stays constant thereafter, since it can be proven that for the measurement of distance to an object at approximately 1 metre, the error due to a temperature change of 10 K is of the order of 1 milimetre (the complete demonstration can be found in Appendix \ref{app:temperature}).

\subsection{Lidar}

LIght Detection and Ranging also works by measuring the time of flight of a signal.
In this case, it is a visible light pulse, usually produced by a laser for its high coherence and low dispersion.

This system is convenient for ranging objects that are small or at a long distance from the sensor, but the small measuring point of the laser beam has some limitations when the mapping of a large area is required.
In these situations, multiple measurements are usually performed sequentially with a rotation of the laser emitter between pulses, but the procedure significantly increases the latency of the system for obstacle detection, and also the complexity of the sensor system itself.

In summary, lidar is a good alternative for ranging, but not so much for detection.

\subsection{Computer vision}

The usage of regular cameras for physical environment sensing is certainly different than the previously considered, since it is not the flight time of a pulsated signal what encodes the information.
Instead, one or more cameras provide a two-dimensional array of data each that can be processed to extract information of distance and location, among others, of any object that is within frame.

It is in that processing where both the advantages and draw backs of computer vision lie.
On one hand, many different transformations can be applied to the stream of data from the cameras, and information can be extracted on very diverse aspects such as colour, luminosity, movement, position, texture\dots \cite{ballard1982} 
Additionally, advanced processing techniques and even artificial intelligence can be applied to the image, which is a very flexible source of information.
On the other hand though, the aforementioned processing tasks should be computed on a relatively high rate video stream, which implies several transformations on various video streams (for stereoscopic vision) with a few million pixels per image, at several frames per second.
Furthermore, if obstacle detection and ranging needs to be performed, more than one viewpoint (and hence more than one camera) are required, and the matching problem (Figure \ref{fig:stereo}) of the objects seen by the two and the geometric transformations are also quite demanding computational problems.
Such a workload can only be dealt with by higher-end computer graphics cards nowadays at a reasonable rate for the effective control of a moving vehicle.

\input{./figures/stereo.tex}

\section{Collision avoidance}

The conventional sense and control problem has two very defined parts: the data acquisition stage that has been studied in Section \ref{sec:sensing} and the actuation of the control variables to modify the state of the system.
In this section, the application alternatives of the latter in TRL 9 (Actual system proven through successful mission operations \cite{assistantsecretaryofdefenseforresearchandengineering2011}) products will be studied.

\subsection{TCAS on conventional aircraft}

The Traffic alert and Collision Avoidance System is the standard system mandated for use by commercial aircraft.
It creates a virtual safety volume around the aircraft, which is based on the time to the Closest Point Approach (CPA) \cite{tooley2009}.

For the system to work, the host aircraft and the threatening aircraft must both equip an ATC transponder.
If the external aircraft has only a Mode A transponder, only Traffic Advisories (TA) can be issued by the TCAS; when it is Mode C or S, also Resolution Advisores (RA) are issued; if the external aircraft is also equipped with a TCAS II, vertical coordination between the two aircraft is additionally provided by means of an ask-answer procedure.

The system relies on two antennas that are usually placed at the top and bottom of the fuselage to provide antenna diversity.
The signal carriers are 1030 MHz radio waves for the asking signal and 1090 MHz for the replying ones.

Regarding the actuation component of the system, it is fully manual.
When a TA or RA are issued, the pilots are responsible to take the control of the aircraft and perform the recommended manoeuvre.
Thus, the system itself cannot be considered to be the one avoiding the collision, but rather helping the higher-level aircraft + crew system accomplish it.

\subsection{DJI Phantom 4}

An Obstacle Collision Avoidance System was not available for commercial UAVs until as recent as March 2016.
Chinese manufacturer DJI unveiled their Phantom 4 emphasising its ``aerial camera'' role for video professionals.

Their reason for including such a system is to facilitate the creative process of filming while the UAV manages the flying aspect of the mission as autonomously as possible.
To that end, apart from the primary recording camera, the Phantom 4 also incorporates two smaller front-facing cameras that use stereoscopic vision algorithms to detect potential obstacles in the planned trajectory and modify it on-the-fly.
In addition, it also incorporates a lower-level failsafe: when the vehicle is not able to compute a reasonable alternative trajectory before the obstacle enters within a predefined distance to the vehicle as measured by the stereoscopic cameras, the flight controller will command a full stop to stationary flight until the obstacle is manually cleared by the pilot.

\input{./figures/phantom.tex}



% \section{Real-time Guidance Navigation and Control}
% 
% \subsection{University of Pennsylvania}
% 
% \subsection{ETH Z\"urich}


